---
layout: post
title:  "Schedule and Talks"
date:   2021-10-10
description: 
tags: [AI, machine learning, autonomous driving, e-mobility, locality, mobility]
comments: true
share: true
authors: [Katharina Anderer]
typora-copy-images-to: ../media/katharina/
typora-root-url: ../
---



#### **Talks:** 

- **Dr. Thomas Grote**: "Ethics and Philosophy Lab" of the Cluster of Excellence "Machine Learning: New Perspectives for Science" at the University of Tübingen. (See Dr. Grote's homepage for further information: [https://sites.google.com/view/thomas-grote/startseite](https://sites.google.com/view/thomas-grote/startseite))

  <u>Keynote talk</u>: "Regulating AI Systems -- Lessons from Clinical Medicine"
  
  
  
- **Prof. Dr. Andreas Jungherr**: holds the Chair for the Governance of Complex and Innovative Technological Systems at the Institute for Political Science at the Otto-Friedrich-University Bamberg. (See Prof. Jungherr's homepage for further information: [https://andreasjungherr.net/](https://andreasjungherr.net/))

  

  

  <u>Keynote talk:</u> "AI and democracy: AI in politics and the politics of AI"

  *As artificial intelligence technology features in ever more aspects of social and economic life, AI becomes political. The use of AI by politicians and states is associated with both hopes and fears. Will AI increase social inequalities and allow politicians to split and manipulate the public? Or will AI allow politicians and states to tackle important problems with better information and new solutions? These fears and hopes find expression in public debate and political contestation. Scholars have an important role in this debate. They are on the frontlines of AI development and its implementation. At the same time, excessive fears or hopes regarding the impact of AI might make for great movie-plots but not so much for good policy advice or ethical guidelines. So, while we need to discuss the potential uses and effects of AI in democracies, we should not be carried away by excessive fears or hopes and thereby accidentally block progress or sow distrust in politics and elections*.
  
- **Prof. Michèle Finck**: Professor of Law and Artificial Intelligence at the University of Tübingen. She focuses on artificial intelligence and the digital economy with a particular emphasis on data (protection) law and governance (See Prof. Finck's homepage for further information: [https://michelefinck.eu/](https://michelefinck.eu/))

  <u>Keynote talk:</u> "The European Union's Regulatory Strategy for Artificial Intelligence"

- **Dr Christopher Burr** is an Ethics Fellow of the [Alan Turing Institute’s Public Policy Programme](https://www.turing.ac.uk/people/researchers/christopher-burr), and the Principal Investigator of a UKRI-funded project, supported by the Trustworthy Autonomous Systems Hub, titled ‘[Ethical Assurance of Digital Mental Healthcare](https://www.turing.ac.uk/research/research-projects/ethical-assurance-digital-mental-healthcare)’. His research expertise includes responsible research and innovation  (with a specific focus on data science and AI) and philosophy of  cognitive science. He works closely with a range of public sector  organisations to help realise more responsible uses of data science and  AI.

   

  <u>Keynote Talk:</u> 'Responsible AI: How to Design, Develop, and Deploy Data-Driven Systems'

   

  *There are a growing number of societal benefits and harms associated with AI. On the one hand, novel machine learning techniques promise to help  tackle climate change or improve clinical research. On the other hand,  recommendation engines can perpetuate hate speech or disinformation on  social media and automated facial recognition systems further exacerbate existing social biases. In an attempt to steer us away from the latter  and towards the former, many have called for more responsible AI  research and innovation.*

  

  *But how do researchers and practitioners identify the responsible uses of  AI from the irresponsible? What separates the morally permissible AI  project from the morally impermissible one?*

  

  *In this lecture, I will offer an intuitive and practical answer to this  question by setting out an approach to responsible scientific research  and technological innovation that is grounded in actual processes AI  design, development, and deployment. Moreover, this approach makes clear why the project of responsible AI is an inherently interdisciplinary  one, requiring input and expertise from a diverse range of backgrounds.  The goal is to present a procedural approach to responsible AI that can  help researchers and developers understand what is required to exercise  responsibility throughout the AI project lifecycle.* 

- **Paul C. Bauer**

  <u>Talk:</u> 'Evaluating survey measures of democratic quality & social cohesion using machine learning tools.'

  *Trust has become one of the foundational concepts of contemporary social theory. Still,*

  *empirical research on trust relies on a relatively small set of measures which are increasingly*
  *debated. Using data from an online, self-administered questionnaire which was conducted*
  *amongst a US representative sample (N = 1,500) and relying on a combination of open-ended*
  *probing data and supervised machine learning, our study compares the validity of*
  *standard measures of generalized social trust with more recent, situation-specific measures of*
  *trust. We find that measures that refer to strangers generally better reflect the conceptual idea*
  *of measuring trust in unknown others. Moreover, situation-specific measures even further*
  *reduce variation in associations, i.e., produce a more similar frame of reference which is*
  *desirable from a measurement perspective. We also present evidence that individuals’*
  *the association may differ in terms of sentiment, independently of the trustee category. Finally, we*
  *end with a discussion of the hard-to-solve challenge of formulating general but not too general*
  *survey measures.*

 



#### Method sessions:



- **Zhijing Jin**: is a Ph.D. at Max Planck Institute & ETH. Her research goals are two-fold: (1) to expand the impact of NLP by promoting NLP for social good, and (2) to improve NLP models by connecting NLP with causal inference. See Zhijing Jin's homepage for further information: https://zhijing-jin.com/fantasy/bio/)

  *Language is the medium for many political activities, from campaigns to  news reports. Natural language processing (NLP) uses computational tools to parse text into key information that is needed for policymaking. In  this session, we will introduce common methods of NLP, including text  classification, topic modeling, event extraction, and text scaling.  Then, we will overview how these methods can be used for policymaking  through four major applications including data collection for  evidence-based policymaking, interpretation of political decisions,  policy communication, and investigation of policy effects. Finally, we  will highlight the challenges of NLP for policymaking in the coming  era.* 



#### Schedule



- Day 1 - Saturday, 29.1.2022: **Mapping the Landscape**

  ​	<u>possible topics</u>

  - intersection points of AI and democracy

  - perils and potentials of AI for democracy

  - public debate: hopes and worries

  - ...

    12:00 Opening and arrival at the workshop 

    13:00 Keynote lecture + Discussion

    14:00 Lunch break

    15:00 contributed talks

    17:00 Coffee break

    17:30 Exchange on methods 

    18:30 Wrap-up and outlook

    19:00 to open end: Social activities

   

  <p>


  </p>

  

- Day 2 - Sunday, 30.1.2022: **Taking responsibility**

  ​	<u>possible topics</u>

  - ethical design of algorithms

  - laws, rules & regulations

  - communication channels from science to politics to society and back

  - ...

    10:00 Keynote lecture + Discussion

    11:00 Exchange on methods

    12:30 contributed and invited talks

    13:30 Lunch break

    14:30 Exchange on methods

    15:30 Coffee break

    16:00 contributed talks

    17:30 BarCamp

    20:30 Conference dinner

<p>


</p>






- Day 3 - Monday, 31.1.2022: **Shaping the Future**

  possible topics

  - developing digital democracy

  - improving political decision

  - enhancing participation and representation

  - mitigating discrimination

  - democratizing AI/ML research itself

  - ...

    10:00 Keynote lecture + Discussion

    11:00 Exchange on methods

    12:30 contributed talks

    13:30 Lunch break

    14:30 Exchange on methods

    15:30 Coffee break

    16:00 contributed talks

    17:15 Wrap-up

    18:00-20:00: Panel discussion open to the general public

 

 

- Departure on Tuesday, 1.2.2022 

  

This section is still in progress and will be updated continually as we are in contact with other speakers. 

Stay tuned! 

 



